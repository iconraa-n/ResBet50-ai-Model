{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"uhhF9Kl3KppJ","executionInfo":{"status":"ok","timestamp":1716387005591,"user_tz":-180,"elapsed":6727,"user":{"displayName":"rolina k","userId":"15953681427659884554"}}},"outputs":[],"source":["import os\n","from os import listdir\n","import cv2\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers, models\n","from tensorflow.keras.utils import to_categorical\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import auc\n","from sklearn.metrics import precision_recall_fscore_support\n","from sklearn.preprocessing import LabelEncoder\n","import re\n","import pathlib"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"wisqOKXCKt1f","colab":{"base_uri":"https://localhost:8080/"},"outputId":"da1c7dee-7e71-4662-b003-2936f201d946","executionInfo":{"status":"ok","timestamp":1716387057601,"user_tz":-180,"elapsed":52057,"user":{"displayName":"rolina k","userId":"15953681427659884554"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ij4ZXJVNKutN","outputId":"dd606af7-1c65-4400-917f-63cf073a801d","executionInfo":{"status":"ok","timestamp":1716387065863,"user_tz":-180,"elapsed":8296,"user":{"displayName":"rolina k","userId":"15953681427659884554"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyheif\n","  Downloading pyheif-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyheif) (1.16.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.0->pyheif) (2.22)\n","Installing collected packages: pyheif\n","Successfully installed pyheif-0.7.1\n"]}],"source":["!pip install pyheif"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"cEZU_UtsSSCC","executionInfo":{"status":"ok","timestamp":1716387146897,"user_tz":-180,"elapsed":81064,"user":{"displayName":"rolina k","userId":"15953681427659884554"}}},"outputs":[],"source":["# Define your dataset directory\n","Agriculture_crop = '/content/drive/MyDrive/crop_images'\n","\n","# Define types\n","types = [\n","    \"jute\", \"maize\", \"rice\", \"sugarcane\", \"wheat\"\n","]\n","\n","# Initialize lists to hold images and their labels\n","CrownPreparationimages = []\n","CrownPreparationlabels = []\n","\n","\n","\n","target_size = (224, 224)  # Desired size to resize images\n","\n","for i, PreparationType in enumerate(types, start=1):\n","    CrownPreparationFolder = os.path.join(Agriculture_crop, str(i))\n","    for root, dirs, files in os.walk(CrownPreparationFolder):\n","        for file_name in files:\n","            img_path = os.path.join(root, file_name)\n","            _, file_extension = os.path.splitext(img_path)\n","            if file_extension.lower() == '.jpeg':\n","            #for add photo output folder\n","            #if file_extension.lower() == '.png':\n","                img = Image.open(img_path)\n","            elif file_extension.lower() == '.heic':\n","                try:\n","                    heif_file = pyheif.read(img_path)\n","                    img = Image.frombytes(\n","                        heif_file.mode,\n","                        heif_file.size,\n","                        heif_file.data,\n","                        \"raw\",\n","                        heif_file.mode,\n","                        heif_file.stride,\n","                    )\n","                except Exception as e:\n","                    print(f\"Error opening {img_path}: {e}\")\n","                    continue\n","            else:\n","                print(f\"Unsupported file format: {img_path}\")\n","                continue\n","\n","            # Resize image to target size\n","            img = img.resize(target_size)\n","\n","            # Append image and label\n","            CrownPreparationimages.append(np.array(img))\n","            CrownPreparationlabels.append(PreparationType)\n","\n","CrownPreparationimages = np.array(CrownPreparationimages)\n","CrownPreparationlabels = np.array(CrownPreparationlabels)"]},{"cell_type":"code","source":["from PIL import Image\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","print(\"Number of images loaded:\", len(CrownPreparationimages))\n","print(\"Number of labels:\", len(CrownPreparationlabels))\n"],"metadata":{"id":"j3j-j0YgXI9t","colab":{"base_uri":"https://localhost:8080/"},"outputId":"32bd41c7-1b13-452c-b31a-8287292fc239","executionInfo":{"status":"ok","timestamp":1716387146908,"user_tz":-180,"elapsed":521,"user":{"displayName":"rolina k","userId":"15953681427659884554"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of images loaded: 804\n","Number of labels: 804\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0CRAazcDK6CS","outputId":"33b240a2-5b4d-43e2-a8db-f3781a723e77"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","21/21 [==============================] - 207s 9s/step - loss: 1.5428 - accuracy: 0.3857 - val_loss: 0.9824 - val_accuracy: 0.6149\n","Epoch 2/100\n","21/21 [==============================] - 171s 8s/step - loss: 0.6580 - accuracy: 0.8025 - val_loss: 0.5998 - val_accuracy: 0.8571\n","Epoch 3/100\n","21/21 [==============================] - 165s 8s/step - loss: 0.4115 - accuracy: 0.8958 - val_loss: 0.4764 - val_accuracy: 0.9006\n","Epoch 4/100\n","21/21 [==============================] - 174s 8s/step - loss: 0.2844 - accuracy: 0.9549 - val_loss: 0.3437 - val_accuracy: 0.9379\n","Epoch 5/100\n","21/21 [==============================] - 194s 9s/step - loss: 0.2075 - accuracy: 0.9813 - val_loss: 0.2813 - val_accuracy: 0.9503\n","Epoch 6/100\n","21/21 [==============================] - 173s 8s/step - loss: 0.1719 - accuracy: 0.9876 - val_loss: 0.2450 - val_accuracy: 0.9441\n","Epoch 7/100\n","21/21 [==============================] - 173s 8s/step - loss: 0.1405 - accuracy: 0.9922 - val_loss: 0.2034 - val_accuracy: 0.9752\n","Epoch 8/100\n","21/21 [==============================] - 179s 9s/step - loss: 0.1138 - accuracy: 0.9953 - val_loss: 0.1772 - val_accuracy: 0.9814\n","Epoch 9/100\n","21/21 [==============================] - 164s 8s/step - loss: 0.0992 - accuracy: 0.9953 - val_loss: 0.1585 - val_accuracy: 0.9876\n","Epoch 10/100\n","21/21 [==============================] - 166s 8s/step - loss: 0.0874 - accuracy: 1.0000 - val_loss: 0.1423 - val_accuracy: 0.9876\n","Epoch 11/100\n","21/21 [==============================] - 173s 8s/step - loss: 0.0740 - accuracy: 0.9984 - val_loss: 0.1303 - val_accuracy: 1.0000\n","Epoch 12/100\n","21/21 [==============================] - 173s 8s/step - loss: 0.0658 - accuracy: 1.0000 - val_loss: 0.1195 - val_accuracy: 1.0000\n","Epoch 13/100\n","21/21 [==============================] - 170s 8s/step - loss: 0.0592 - accuracy: 1.0000 - val_loss: 0.1115 - val_accuracy: 1.0000\n","Epoch 14/100\n","21/21 [==============================] - 173s 8s/step - loss: 0.0538 - accuracy: 1.0000 - val_loss: 0.0993 - val_accuracy: 1.0000\n","Epoch 15/100\n","21/21 [==============================] - 165s 8s/step - loss: 0.0477 - accuracy: 1.0000 - val_loss: 0.0969 - val_accuracy: 1.0000\n","Epoch 16/100\n","21/21 [==============================] - 173s 8s/step - loss: 0.0432 - accuracy: 1.0000 - val_loss: 0.0856 - val_accuracy: 1.0000\n","Epoch 17/100\n","21/21 [==============================] - 173s 8s/step - loss: 0.0402 - accuracy: 1.0000 - val_loss: 0.0839 - val_accuracy: 1.0000\n","Epoch 18/100\n","21/21 [==============================] - 171s 8s/step - loss: 0.0369 - accuracy: 1.0000 - val_loss: 0.0758 - val_accuracy: 1.0000\n","Epoch 19/100\n","21/21 [==============================] - 171s 8s/step - loss: 0.0345 - accuracy: 1.0000 - val_loss: 0.0716 - val_accuracy: 1.0000\n","Epoch 20/100\n","21/21 [==============================] - 175s 8s/step - loss: 0.0319 - accuracy: 1.0000 - val_loss: 0.0723 - val_accuracy: 1.0000\n","Epoch 21/100\n","21/21 [==============================] - 162s 8s/step - loss: 0.0291 - accuracy: 1.0000 - val_loss: 0.0645 - val_accuracy: 1.0000\n","Epoch 22/100\n","21/21 [==============================] - 171s 8s/step - loss: 0.0267 - accuracy: 1.0000 - val_loss: 0.0622 - val_accuracy: 1.0000\n","Epoch 23/100\n","21/21 [==============================] - 171s 8s/step - loss: 0.0250 - accuracy: 1.0000 - val_loss: 0.0565 - val_accuracy: 1.0000\n","Epoch 24/100\n","21/21 [==============================] - 164s 8s/step - loss: 0.0237 - accuracy: 1.0000 - val_loss: 0.0552 - val_accuracy: 1.0000\n","Epoch 25/100\n","21/21 [==============================] - 170s 8s/step - loss: 0.0221 - accuracy: 1.0000 - val_loss: 0.0513 - val_accuracy: 1.0000\n","Epoch 26/100\n","21/21 [==============================] - 171s 8s/step - loss: 0.0207 - accuracy: 1.0000 - val_loss: 0.0505 - val_accuracy: 1.0000\n","Epoch 27/100\n","21/21 [==============================] - 171s 8s/step - loss: 0.0197 - accuracy: 1.0000 - val_loss: 0.0487 - val_accuracy: 1.0000\n","Epoch 28/100\n","21/21 [==============================] - 170s 8s/step - loss: 0.0186 - accuracy: 1.0000 - val_loss: 0.0473 - val_accuracy: 1.0000\n","Epoch 29/100\n","21/21 [==============================] - 169s 8s/step - loss: 0.0175 - accuracy: 1.0000 - val_loss: 0.0455 - val_accuracy: 1.0000\n","Epoch 30/100\n","21/21 [==============================] - 169s 8s/step - loss: 0.0169 - accuracy: 1.0000 - val_loss: 0.0426 - val_accuracy: 1.0000\n","Epoch 31/100\n","21/21 [==============================] - 172s 8s/step - loss: 0.0157 - accuracy: 1.0000 - val_loss: 0.0430 - val_accuracy: 1.0000\n","Epoch 32/100\n","21/21 [==============================] - 171s 8s/step - loss: 0.0152 - accuracy: 1.0000 - val_loss: 0.0408 - val_accuracy: 1.0000\n","Epoch 33/100\n","21/21 [==============================] - 172s 8s/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 0.0412 - val_accuracy: 1.0000\n","Epoch 34/100\n","21/21 [==============================] - 173s 8s/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 0.0381 - val_accuracy: 1.0000\n","Epoch 35/100\n","21/21 [==============================] - 171s 8s/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.0377 - val_accuracy: 1.0000\n","Epoch 36/100\n","21/21 [==============================] - 171s 8s/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.0366 - val_accuracy: 1.0000\n","Epoch 37/100\n","21/21 [==============================] - 172s 8s/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.0353 - val_accuracy: 1.0000\n","Epoch 38/100\n","21/21 [==============================] - 172s 8s/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.0337 - val_accuracy: 1.0000\n","Epoch 39/100\n","21/21 [==============================] - 173s 8s/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.0337 - val_accuracy: 1.0000\n","Epoch 40/100\n","21/21 [==============================] - 172s 8s/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.0327 - val_accuracy: 1.0000\n","Epoch 41/100\n","21/21 [==============================] - 171s 8s/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.0326 - val_accuracy: 1.0000\n","Epoch 42/100\n","21/21 [==============================] - 171s 8s/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.0317 - val_accuracy: 1.0000\n","Epoch 43/100\n","21/21 [==============================] - 163s 8s/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.0304 - val_accuracy: 1.0000\n","Epoch 44/100\n","21/21 [==============================] - 172s 8s/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.0304 - val_accuracy: 1.0000\n","Epoch 45/100\n","21/21 [==============================] - 171s 8s/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.0292 - val_accuracy: 1.0000\n","Epoch 46/100\n","21/21 [==============================] - 171s 8s/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.0275 - val_accuracy: 1.0000\n","Epoch 47/100\n","21/21 [==============================] - 171s 8s/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.0272 - val_accuracy: 1.0000\n","Epoch 48/100\n","21/21 [==============================] - 170s 8s/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.0265 - val_accuracy: 1.0000\n","Epoch 49/100\n","21/21 [==============================] - 171s 8s/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.0258 - val_accuracy: 1.0000\n","Epoch 50/100\n","21/21 [==============================] - 171s 8s/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.0256 - val_accuracy: 1.0000\n","Epoch 51/100\n","21/21 [==============================] - 163s 8s/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.0260 - val_accuracy: 1.0000\n","Epoch 52/100\n","21/21 [==============================] - 170s 8s/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.0240 - val_accuracy: 1.0000\n","Epoch 53/100\n","21/21 [==============================] - 171s 8s/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0240 - val_accuracy: 1.0000\n","Epoch 54/100\n","21/21 [==============================] - 171s 8s/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 1.0000\n","Epoch 55/100\n","21/21 [==============================] - 171s 8s/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 1.0000\n","Epoch 56/100\n","21/21 [==============================] - 170s 8s/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 1.0000\n","Epoch 57/100\n","21/21 [==============================] - 170s 8s/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 1.0000\n","Epoch 58/100\n","21/21 [==============================] - 170s 8s/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 1.0000\n","Epoch 59/100\n","21/21 [==============================] - 169s 8s/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.0219 - val_accuracy: 1.0000\n","Epoch 60/100\n","21/21 [==============================] - 171s 8s/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.0216 - val_accuracy: 1.0000\n","Epoch 61/100\n","21/21 [==============================] - 168s 8s/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.0213 - val_accuracy: 1.0000\n","Epoch 62/100\n","21/21 [==============================] - 170s 8s/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.0209 - val_accuracy: 1.0000\n","Epoch 63/100\n","21/21 [==============================] - 170s 8s/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.0210 - val_accuracy: 1.0000\n","Epoch 64/100\n","21/21 [==============================] - 170s 8s/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.0202 - val_accuracy: 1.0000\n","Epoch 65/100\n","21/21 [==============================] - 164s 8s/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0200 - val_accuracy: 1.0000\n","Epoch 66/100\n","21/21 [==============================] - 170s 8s/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.0201 - val_accuracy: 1.0000\n","Epoch 67/100\n","21/21 [==============================] - 171s 8s/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.0193 - val_accuracy: 1.0000\n","Epoch 68/100\n","19/21 [==========================>...] - ETA: 13s - loss: 0.0043 - accuracy: 1.0000"]}],"source":["# Define the ResNet model\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelBinarizer\n","from tensorflow.keras import layers, models\n","from tensorflow.keras.callbacks import EarlyStopping\n","from sklearn.svm import SVC\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, Dense\n","\n","\n","# Split the dataset into training and testing\n","train_images, test_images, train_labels, test_labels = train_test_split(\n","    CrownPreparationimages, CrownPreparationlabels, test_size=0.2, random_state=42)\n","\n","# Convert labels to one-hot encoding\n","label_binarizer = LabelBinarizer()\n","train_labels = label_binarizer.fit_transform(train_labels)\n","test_labels = label_binarizer.transform(test_labels)\n","\n","img_width, img_height = 224, 224\n","batch_size = 16\n","\n","early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n","\n","def build_resnet(input_shape, num_classes):\n","    resnet = tf.keras.applications.ResNet50(\n","        include_top=False,\n","        weights='imagenet',\n","        input_shape=input_shape\n","    )\n","\n","    for layer in resnet.layers:\n","        layer.trainable = False\n","\n","    model = models.Sequential([\n","        resnet,\n","        layers.GlobalAveragePooling2D(),\n","        layers.Dense(num_classes, activation='softmax')\n","    ])\n","\n","    return model\n","\n","# Build the model\n","model = build_resnet((img_width, img_height, 3), num_classes=len(types))\n","\n","# Compile the model\n","model.compile(optimizer='adam',\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Train the model\n","history = model.fit(train_images, train_labels, epochs=100,\n","                    validation_data=(test_images, test_labels),\n","                    callbacks=[early_stopping])\n","\n","# Evaluate the model\n","test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n","print('\\nTest accuracy:', test_acc)"]}],"metadata":{"colab":{"provenance":[{"file_id":"1Cvp22w-lrVe-yVXhJLBR4mhD0LI4_IGr","timestamp":1716386889993}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}